{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Tutorial\n",
    "## Written by Tim Dell\n",
    " Modified with permission from Erik Hallstr√∂m.\n",
    " Medium post 'How to build RNN in tensorflow' series.\n",
    "https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will look at how to build a simple Recurrent Neural network using the tensorflow API for RNNs. \n",
    "\n",
    "RNNs are a neural network used when the data is a sequence or a series of data points. This sequence does not have a fixed size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import the required libraries. Tensorflow, Numpy and Matplotlib for viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up the variables that we would like to be able to modify in the future. Epoch, number of iterations of learning. Series length, classes, batch size etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the data that we want to train on. In this tutorial we are going to create a series of random variables and have the recurrent network regenerate the input series a couple of time steps later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p = [0.5,0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "    \n",
    "    \n",
    "    x = x.reshape((batch_size, -1))\n",
    "    y = y.reshape((batch_size, -1))\n",
    "    \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this we set up placeholders for the batches and the initial state. The placeholders are used to feed in the data to the RNN when the session begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up the changeable variables. Using tf.Variable this initilisation allows the session to access and train on the weights and biases of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype = tf.float32)\n",
    "b = tf.Variable(np.zeros((1,state_size)), dtype = tf.float32)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes), dtype = tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)), dtype = tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are unstacked to be fed into the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unpack columns\n",
    "\n",
    "input_series = tf.unstack(batchX_placeholder, axis = 1)\n",
    "label_series = tf.unstack(batchY_placeholder, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to set up the forward pass.In a vanilla RNN the forward pass needs to be unrolled with a for loop and each state needs to be calculated. Later it will be shown that the tensorflow API provides a rolled out RNN inbuilt in the library. It is seen that the system is reshaped to be fed into a tanh activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Forward Pass\n",
    "\n",
    "current_state = init_state\n",
    "states_series = []\n",
    "\n",
    "for current_input in input_series:\n",
    "    current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "    input_and_state_concat = tf.concat( [current_input, current_state], 1)\n",
    "    \n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concat, W) + b)\n",
    "    states_series.append(next_state)\n",
    "    current_state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we set up the logits and predictions of the series and apply a softmax layer to the results.\n",
    "The system then initialises the error function using reduce_mean and the gradient descent algorithm, seen here with the inbuilt adagrad optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series]\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits,labels = labels) for logits, labels in zip(logits_series,label_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a function to plot the results of the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "    \n",
    "    \n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "        \n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width = 1, color = \"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width = 1, color = \"red\" )\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width = 1, color = \"green\")\n",
    "        \n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we initialise the actual training with the with tf.Session() line. The system then runs through the epochs  and generates the input data, nest it runs the training across the data and produces the loss and the predictions. The system then outputs on the display an indicator wshowing the step and the loss at an interval of every 100 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11628da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Data, epoch 0\n",
      "Step 0 Loss 0.701099\n",
      "Step 100 Loss 0.693073\n",
      "Step 200 Loss 0.689492\n",
      "Step 300 Loss 0.693039\n",
      "Step 400 Loss 0.697603\n",
      "Step 500 Loss 0.701983\n",
      "Step 600 Loss 0.692726\n",
      "New Data, epoch 1\n",
      "Step 0 Loss 0.693576\n",
      "Step 100 Loss 0.691514\n",
      "Step 200 Loss 0.712362\n",
      "Step 300 Loss 0.687532\n",
      "Step 400 Loss 0.694719\n",
      "Step 500 Loss 0.693438\n",
      "Step 600 Loss 0.691848\n",
      "New Data, epoch 2\n",
      "Step 0 Loss 0.695305\n",
      "Step 100 Loss 0.694315\n",
      "Step 200 Loss 0.699786\n",
      "Step 300 Loss 0.708219\n",
      "Step 400 Loss 0.690003\n",
      "Step 500 Loss 0.692914\n",
      "Step 600 Loss 0.69462\n",
      "New Data, epoch 3\n",
      "Step 0 Loss 0.709068\n",
      "Step 100 Loss 0.693923\n",
      "Step 200 Loss 0.693917\n",
      "Step 300 Loss 0.694533\n",
      "Step 400 Loss 0.693444\n",
      "Step 500 Loss 0.692315\n",
      "Step 600 Loss 0.695702\n",
      "New Data, epoch 4\n",
      "Step 0 Loss 0.702585\n",
      "Step 100 Loss 0.692496\n",
      "Step 200 Loss 0.692783\n",
      "Step 300 Loss 0.693353\n",
      "Step 400 Loss 0.701668\n",
      "Step 500 Loss 0.698014\n",
      "Step 600 Loss 0.693031\n",
      "New Data, epoch 5\n",
      "Step 0 Loss 0.692995\n",
      "Step 100 Loss 0.69309\n",
      "Step 200 Loss 0.692706\n",
      "Step 300 Loss 0.689929\n",
      "Step 400 Loss 0.693479\n",
      "Step 500 Loss 0.696153\n",
      "Step 600 Loss 0.691495\n",
      "New Data, epoch 6\n",
      "Step 0 Loss 0.696416\n",
      "Step 100 Loss 0.694632\n",
      "Step 200 Loss 0.685887\n",
      "Step 300 Loss 0.692591\n",
      "Step 400 Loss 0.689431\n",
      "Step 500 Loss 0.702016\n",
      "Step 600 Loss 0.69142\n",
      "New Data, epoch 7\n",
      "Step 0 Loss 0.697987\n",
      "Step 100 Loss 0.693819\n",
      "Step 200 Loss 0.693467\n",
      "Step 300 Loss 0.692413\n",
      "Step 400 Loss 0.691488\n",
      "Step 500 Loss 0.692194\n",
      "Step 600 Loss 0.692752\n",
      "New Data, epoch 8\n",
      "Step 0 Loss 0.693327\n",
      "Step 100 Loss 0.691048\n",
      "Step 200 Loss 0.692634\n",
      "Step 300 Loss 0.695491\n",
      "Step 400 Loss 0.693719\n",
      "Step 500 Loss 0.693622\n",
      "Step 600 Loss 0.693604\n",
      "New Data, epoch 9\n",
      "Step 0 Loss 0.69977\n",
      "Step 100 Loss 0.693541\n",
      "Step 200 Loss 0.6918\n",
      "Step 300 Loss 0.692915\n",
      "Step 400 Loss 0.695685\n",
      "Step 500 Loss 0.692979\n",
      "Step 600 Loss 0.693336\n",
      "New Data, epoch 10\n",
      "Step 0 Loss 0.687011\n",
      "Step 100 Loss 0.691174\n",
      "Step 200 Loss 0.695839\n",
      "Step 300 Loss 0.700566\n",
      "Step 400 Loss 0.693152\n",
      "Step 500 Loss 0.689275\n",
      "Step 600 Loss 0.695714\n",
      "New Data, epoch 11\n",
      "Step 0 Loss 0.694596\n",
      "Step 100 Loss 0.693302\n",
      "Step 200 Loss 0.688555\n",
      "Step 300 Loss 0.694537\n",
      "Step 400 Loss 0.696644\n",
      "Step 500 Loss 0.692701\n",
      "Step 600 Loss 0.686305\n",
      "New Data, epoch 12\n",
      "Step 0 Loss 0.695246\n",
      "Step 100 Loss 0.69469\n",
      "Step 200 Loss 0.69596\n",
      "Step 300 Loss 0.692382\n",
      "Step 400 Loss 0.693036\n",
      "Step 500 Loss 0.690812\n",
      "Step 600 Loss 0.689896\n",
      "New Data, epoch 13\n",
      "Step 0 Loss 0.694214\n",
      "Step 100 Loss 0.694815\n",
      "Step 200 Loss 0.692473\n",
      "Step 300 Loss 0.691207\n",
      "Step 400 Loss 0.694448\n",
      "Step 500 Loss 0.693657\n",
      "Step 600 Loss 0.693193\n",
      "New Data, epoch 14\n",
      "Step 0 Loss 0.693903\n",
      "Step 100 Loss 0.697611\n",
      "Step 200 Loss 0.692978\n",
      "Step 300 Loss 0.694802\n",
      "Step 400 Loss 0.691927\n",
      "Step 500 Loss 0.69727\n",
      "Step 600 Loss 0.693063\n",
      "New Data, epoch 15\n",
      "Step 0 Loss 0.697281\n",
      "Step 100 Loss 0.697762\n",
      "Step 200 Loss 0.692382\n",
      "Step 300 Loss 0.694585\n",
      "Step 400 Loss 0.694914\n",
      "Step 500 Loss 0.694415\n",
      "Step 600 Loss 0.693852\n",
      "New Data, epoch 16\n",
      "Step 0 Loss 0.697469\n",
      "Step 100 Loss 0.699949\n",
      "Step 200 Loss 0.693707\n",
      "Step 300 Loss 0.6958\n",
      "Step 400 Loss 0.695379\n",
      "Step 500 Loss 0.697854\n",
      "Step 600 Loss 0.695334\n",
      "New Data, epoch 17\n",
      "Step 0 Loss 0.683834\n",
      "Step 100 Loss 0.698412\n",
      "Step 200 Loss 0.692606\n",
      "Step 300 Loss 0.693255\n",
      "Step 400 Loss 0.694877\n",
      "Step 500 Loss 0.691099\n",
      "Step 600 Loss 0.694189\n",
      "New Data, epoch 18\n",
      "Step 0 Loss 0.695308\n",
      "Step 100 Loss 0.691718\n",
      "Step 200 Loss 0.691813\n",
      "Step 300 Loss 0.693066\n",
      "Step 400 Loss 0.692128\n",
      "Step 500 Loss 0.693213\n",
      "Step 600 Loss 0.697694\n",
      "New Data, epoch 19\n",
      "Step 0 Loss 0.691435\n",
      "Step 100 Loss 0.693072\n",
      "Step 200 Loss 0.693072\n",
      "Step 300 Loss 0.693855\n",
      "Step 400 Loss 0.693741\n",
      "Step 500 Loss 0.692773\n",
      "Step 600 Loss 0.693807\n",
      "New Data, epoch 20\n",
      "Step 0 Loss 0.695754\n",
      "Step 100 Loss 0.696227\n",
      "Step 200 Loss 0.696799\n",
      "Step 300 Loss 0.692369\n",
      "Step 400 Loss 0.692551\n",
      "Step 500 Loss 0.693449\n",
      "Step 600 Loss 0.69798\n",
      "New Data, epoch 21\n",
      "Step 0 Loss 0.693999\n",
      "Step 100 Loss 0.693264\n",
      "Step 200 Loss 0.697073\n",
      "Step 300 Loss 0.695703\n",
      "Step 400 Loss 0.693385\n",
      "Step 500 Loss 0.694134\n",
      "Step 600 Loss 0.692324\n",
      "New Data, epoch 22\n",
      "Step 0 Loss 0.68963\n",
      "Step 100 Loss 0.693143\n",
      "Step 200 Loss 0.690678\n",
      "Step 300 Loss 0.691787\n",
      "Step 400 Loss 0.69632\n",
      "Step 500 Loss 0.693461\n",
      "Step 600 Loss 0.700441\n",
      "New Data, epoch 23\n",
      "Step 0 Loss 0.690039\n",
      "Step 100 Loss 0.702005\n",
      "Step 200 Loss 0.686387\n",
      "Step 300 Loss 0.69077\n",
      "Step 400 Loss 0.690439\n",
      "Step 500 Loss 0.69312\n",
      "Step 600 Loss 0.694028\n",
      "New Data, epoch 24\n",
      "Step 0 Loss 0.695302\n",
      "Step 100 Loss 0.693106\n",
      "Step 200 Loss 0.693486\n",
      "Step 300 Loss 0.69413\n",
      "Step 400 Loss 0.693496\n",
      "Step 500 Loss 0.688\n",
      "Step 600 Loss 0.693688\n",
      "New Data, epoch 25\n",
      "Step 0 Loss 0.695001\n",
      "Step 100 Loss 0.690922\n",
      "Step 200 Loss 0.68753\n",
      "Step 300 Loss 0.692511\n",
      "Step 400 Loss 0.696042\n",
      "Step 500 Loss 0.69618\n",
      "Step 600 Loss 0.693114\n",
      "New Data, epoch 26\n",
      "Step 0 Loss 0.698212\n",
      "Step 100 Loss 0.695146\n",
      "Step 200 Loss 0.693058\n",
      "Step 300 Loss 0.693068\n",
      "Step 400 Loss 0.701936\n",
      "Step 500 Loss 0.700061\n",
      "Step 600 Loss 0.695679\n",
      "New Data, epoch 27\n",
      "Step 0 Loss 0.692177\n",
      "Step 100 Loss 0.689768\n",
      "Step 200 Loss 0.698539\n",
      "Step 300 Loss 0.694709\n",
      "Step 400 Loss 0.692904\n",
      "Step 500 Loss 0.698187\n",
      "Step 600 Loss 0.691624\n",
      "New Data, epoch 28\n",
      "Step 0 Loss 0.690439\n",
      "Step 100 Loss 0.696993\n",
      "Step 200 Loss 0.696625\n",
      "Step 300 Loss 0.691919\n",
      "Step 400 Loss 0.693142\n",
      "Step 500 Loss 0.696517\n",
      "Step 600 Loss 0.692641\n",
      "New Data, epoch 29\n",
      "Step 0 Loss 0.695567\n",
      "Step 100 Loss 0.691335\n",
      "Step 200 Loss 0.690323\n",
      "Step 300 Loss 0.693107\n",
      "Step 400 Loss 0.693426\n",
      "Step 500 Loss 0.693198\n",
      "Step 600 Loss 0.687941\n",
      "New Data, epoch 30\n",
      "Step 0 Loss 0.694381\n",
      "Step 100 Loss 0.693091\n",
      "Step 200 Loss 0.692671\n",
      "Step 300 Loss 0.690018\n",
      "Step 400 Loss 0.692882\n",
      "Step 500 Loss 0.6984\n",
      "Step 600 Loss 0.69199\n",
      "New Data, epoch 31\n",
      "Step 0 Loss 0.692672\n",
      "Step 100 Loss 0.693147\n",
      "Step 200 Loss 0.693144\n",
      "Step 300 Loss 0.69245\n",
      "Step 400 Loss 0.695184\n",
      "Step 500 Loss 0.693134\n",
      "Step 600 Loss 0.698167\n",
      "New Data, epoch 32\n",
      "Step 0 Loss 0.690489\n",
      "Step 100 Loss 0.694081\n",
      "Step 200 Loss 0.691996\n",
      "Step 300 Loss 0.696043\n",
      "Step 400 Loss 0.693119\n",
      "Step 500 Loss 0.692668\n",
      "Step 600 Loss 0.693197\n",
      "New Data, epoch 33\n",
      "Step 0 Loss 0.69491\n",
      "Step 100 Loss 0.695331\n",
      "Step 200 Loss 0.693134\n",
      "Step 300 Loss 0.697531\n",
      "Step 400 Loss 0.688858\n",
      "Step 500 Loss 0.694256\n",
      "Step 600 Loss 0.705813\n",
      "New Data, epoch 34\n",
      "Step 0 Loss 0.693555\n",
      "Step 100 Loss 0.697002\n",
      "Step 200 Loss 0.693149\n",
      "Step 300 Loss 0.693432\n",
      "Step 400 Loss 0.695313\n",
      "Step 500 Loss 0.692005\n",
      "Step 600 Loss 0.69343\n",
      "New Data, epoch 35\n",
      "Step 0 Loss 0.701604\n",
      "Step 100 Loss 0.69419\n",
      "Step 200 Loss 0.692944\n",
      "Step 300 Loss 0.693252\n",
      "Step 400 Loss 0.691902\n",
      "Step 500 Loss 0.693778\n",
      "Step 600 Loss 0.69628\n",
      "New Data, epoch 36\n",
      "Step 0 Loss 0.695471\n",
      "Step 100 Loss 0.692846\n",
      "Step 200 Loss 0.696593\n",
      "Step 300 Loss 0.697538\n",
      "Step 400 Loss 0.693591\n",
      "Step 500 Loss 0.690539\n",
      "Step 600 Loss 0.692557\n",
      "New Data, epoch 37\n",
      "Step 0 Loss 0.695289\n",
      "Step 100 Loss 0.697301\n",
      "Step 200 Loss 0.693251\n",
      "Step 300 Loss 0.694625\n",
      "Step 400 Loss 0.698648\n",
      "Step 500 Loss 0.691099\n",
      "Step 600 Loss 0.694739\n",
      "New Data, epoch 38\n",
      "Step 0 Loss 0.693478\n",
      "Step 100 Loss 0.693605\n",
      "Step 200 Loss 0.694254\n",
      "Step 300 Loss 0.693785\n",
      "Step 400 Loss 0.692585\n",
      "Step 500 Loss 0.692747\n",
      "Step 600 Loss 0.690905\n",
      "New Data, epoch 39\n",
      "Step 0 Loss 0.688939\n",
      "Step 100 Loss 0.698599\n",
      "Step 200 Loss 0.692666\n",
      "Step 300 Loss 0.691642\n",
      "Step 400 Loss 0.704448\n",
      "Step 500 Loss 0.692918\n",
      "Step 600 Loss 0.694939\n",
      "New Data, epoch 40\n",
      "Step 0 Loss 0.693009\n",
      "Step 100 Loss 0.693682\n",
      "Step 200 Loss 0.693158\n",
      "Step 300 Loss 0.693079\n",
      "Step 400 Loss 0.693256\n",
      "Step 500 Loss 0.689989\n",
      "Step 600 Loss 0.696917\n",
      "New Data, epoch 41\n",
      "Step 0 Loss 0.69306\n",
      "Step 100 Loss 0.692395\n",
      "Step 200 Loss 0.693277\n",
      "Step 300 Loss 0.693893\n",
      "Step 400 Loss 0.692222\n",
      "Step 500 Loss 0.692174\n",
      "Step 600 Loss 0.694155\n",
      "New Data, epoch 42\n",
      "Step 0 Loss 0.69322\n",
      "Step 100 Loss 0.683951\n",
      "Step 200 Loss 0.688782\n",
      "Step 300 Loss 0.69424\n",
      "Step 400 Loss 0.701094\n",
      "Step 500 Loss 0.687526\n",
      "Step 600 Loss 0.6932\n",
      "New Data, epoch 43\n",
      "Step 0 Loss 0.692702\n",
      "Step 100 Loss 0.693006\n",
      "Step 200 Loss 0.686808\n",
      "Step 300 Loss 0.692983\n",
      "Step 400 Loss 0.693052\n",
      "Step 500 Loss 0.692022\n",
      "Step 600 Loss 0.6933\n",
      "New Data, epoch 44\n",
      "Step 0 Loss 0.690329\n",
      "Step 100 Loss 0.692298\n",
      "Step 200 Loss 0.693061\n",
      "Step 300 Loss 0.693801\n",
      "Step 400 Loss 0.693496\n",
      "Step 500 Loss 0.690278\n",
      "Step 600 Loss 0.695524\n",
      "New Data, epoch 45\n",
      "Step 0 Loss 0.698024\n",
      "Step 100 Loss 0.69241\n",
      "Step 200 Loss 0.693342\n",
      "Step 300 Loss 0.693304\n",
      "Step 400 Loss 0.692751\n",
      "Step 500 Loss 0.697252\n",
      "Step 600 Loss 0.690308\n",
      "New Data, epoch 46\n",
      "Step 0 Loss 0.690704\n",
      "Step 100 Loss 0.693215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 Loss 0.693014\n",
      "Step 300 Loss 0.694316\n",
      "Step 400 Loss 0.694904\n",
      "Step 500 Loss 0.699457\n",
      "Step 600 Loss 0.694532\n",
      "New Data, epoch 47\n",
      "Step 0 Loss 0.694398\n",
      "Step 100 Loss 0.693079\n",
      "Step 200 Loss 0.693809\n",
      "Step 300 Loss 0.692751\n",
      "Step 400 Loss 0.699662\n",
      "Step 500 Loss 0.689977\n",
      "Step 600 Loss 0.694662\n",
      "New Data, epoch 48\n",
      "Step 0 Loss 0.694952\n",
      "Step 100 Loss 0.700122\n",
      "Step 200 Loss 0.693139\n",
      "Step 300 Loss 0.691965\n",
      "Step 400 Loss 0.692501\n",
      "Step 500 Loss 0.696215\n",
      "Step 600 Loss 0.697282\n",
      "New Data, epoch 49\n",
      "Step 0 Loss 0.691778\n",
      "Step 100 Loss 0.693837\n",
      "Step 200 Loss 0.703508\n",
      "Step 300 Loss 0.693049\n",
      "Step 400 Loss 0.693079\n",
      "Step 500 Loss 0.690323\n",
      "Step 600 Loss 0.693523\n",
      "New Data, epoch 50\n",
      "Step 0 Loss 0.69334\n",
      "Step 100 Loss 0.696094\n",
      "Step 200 Loss 0.692605\n",
      "Step 300 Loss 0.689397\n",
      "Step 400 Loss 0.689335\n",
      "Step 500 Loss 0.694494\n",
      "Step 600 Loss 0.693343\n",
      "New Data, epoch 51\n",
      "Step 0 Loss 0.691517\n",
      "Step 100 Loss 0.697145\n",
      "Step 200 Loss 0.692491\n",
      "Step 300 Loss 0.692539\n",
      "Step 400 Loss 0.693365\n",
      "Step 500 Loss 0.693487\n",
      "Step 600 Loss 0.692865\n",
      "New Data, epoch 52\n",
      "Step 0 Loss 0.697605\n",
      "Step 100 Loss 0.693131\n",
      "Step 200 Loss 0.695273\n",
      "Step 300 Loss 0.693799\n",
      "Step 400 Loss 0.691843\n",
      "Step 500 Loss 0.693301\n",
      "Step 600 Loss 0.692741\n",
      "New Data, epoch 53\n",
      "Step 0 Loss 0.696469\n",
      "Step 100 Loss 0.691769\n",
      "Step 200 Loss 0.693133\n",
      "Step 300 Loss 0.692753\n",
      "Step 400 Loss 0.693121\n",
      "Step 500 Loss 0.692653\n",
      "Step 600 Loss 0.689781\n",
      "New Data, epoch 54\n",
      "Step 0 Loss 0.699756\n",
      "Step 100 Loss 0.69273\n",
      "Step 200 Loss 0.689494\n",
      "Step 300 Loss 0.692857\n",
      "Step 400 Loss 0.693517\n",
      "Step 500 Loss 0.693013\n",
      "Step 600 Loss 0.694359\n",
      "New Data, epoch 55\n",
      "Step 0 Loss 0.692188\n",
      "Step 100 Loss 0.687416\n",
      "Step 200 Loss 0.693516\n",
      "Step 300 Loss 0.692962\n",
      "Step 400 Loss 0.694743\n",
      "Step 500 Loss 0.692623\n",
      "Step 600 Loss 0.695226\n",
      "New Data, epoch 56\n",
      "Step 0 Loss 0.694333\n",
      "Step 100 Loss 0.695351\n",
      "Step 200 Loss 0.69304\n",
      "Step 300 Loss 0.693398\n",
      "Step 400 Loss 0.693128\n",
      "Step 500 Loss 0.691745\n",
      "Step 600 Loss 0.693843\n",
      "New Data, epoch 57\n",
      "Step 0 Loss 0.691938\n",
      "Step 100 Loss 0.695461\n",
      "Step 200 Loss 0.688986\n",
      "Step 300 Loss 0.69432\n",
      "Step 400 Loss 0.693311\n",
      "Step 500 Loss 0.692864\n",
      "Step 600 Loss 0.69306\n",
      "New Data, epoch 58\n",
      "Step 0 Loss 0.692979\n",
      "Step 100 Loss 0.690463\n",
      "Step 200 Loss 0.69302\n",
      "Step 300 Loss 0.692476\n",
      "Step 400 Loss 0.693067\n",
      "Step 500 Loss 0.689876\n",
      "Step 600 Loss 0.693228\n",
      "New Data, epoch 59\n",
      "Step 0 Loss 0.694684\n",
      "Step 100 Loss 0.694114\n",
      "Step 200 Loss 0.694134\n",
      "Step 300 Loss 0.696813\n",
      "Step 400 Loss 0.692446\n",
      "Step 500 Loss 0.692834\n",
      "Step 600 Loss 0.693879\n",
      "New Data, epoch 60\n",
      "Step 0 Loss 0.684113\n",
      "Step 100 Loss 0.693072\n",
      "Step 200 Loss 0.697964\n",
      "Step 300 Loss 0.693516\n",
      "Step 400 Loss 0.689982\n",
      "Step 500 Loss 0.689895\n",
      "Step 600 Loss 0.693173\n",
      "New Data, epoch 61\n",
      "Step 0 Loss 0.692029\n",
      "Step 100 Loss 0.692533\n",
      "Step 200 Loss 0.691963\n",
      "Step 300 Loss 0.694679\n",
      "Step 400 Loss 0.692983\n",
      "Step 500 Loss 0.69924\n",
      "Step 600 Loss 0.693623\n",
      "New Data, epoch 62\n",
      "Step 0 Loss 0.68674\n",
      "Step 100 Loss 0.692829\n",
      "Step 200 Loss 0.693186\n",
      "Step 300 Loss 0.693157\n",
      "Step 400 Loss 0.692645\n",
      "Step 500 Loss 0.6947\n",
      "Step 600 Loss 0.693306\n",
      "New Data, epoch 63\n",
      "Step 0 Loss 0.692247\n",
      "Step 100 Loss 0.695033\n",
      "Step 200 Loss 0.685736\n",
      "Step 300 Loss 0.688545\n",
      "Step 400 Loss 0.692981\n",
      "Step 500 Loss 0.691625\n",
      "Step 600 Loss 0.694097\n",
      "New Data, epoch 64\n",
      "Step 0 Loss 0.693792\n",
      "Step 100 Loss 0.693095\n",
      "Step 200 Loss 0.696684\n",
      "Step 300 Loss 0.692728\n",
      "Step 400 Loss 0.692832\n",
      "Step 500 Loss 0.694396\n",
      "Step 600 Loss 0.691425\n",
      "New Data, epoch 65\n",
      "Step 0 Loss 0.694966\n",
      "Step 100 Loss 0.694508\n",
      "Step 200 Loss 0.693671\n",
      "Step 300 Loss 0.690265\n",
      "Step 400 Loss 0.690931\n",
      "Step 500 Loss 0.692596\n",
      "Step 600 Loss 0.692487\n",
      "New Data, epoch 66\n",
      "Step 0 Loss 0.69016\n",
      "Step 100 Loss 0.691637\n",
      "Step 200 Loss 0.691376\n",
      "Step 300 Loss 0.689151\n",
      "Step 400 Loss 0.693299\n",
      "Step 500 Loss 0.693096\n",
      "Step 600 Loss 0.691405\n",
      "New Data, epoch 67\n",
      "Step 0 Loss 0.691419\n",
      "Step 100 Loss 0.692797\n",
      "Step 200 Loss 0.694007\n",
      "Step 300 Loss 0.693005\n",
      "Step 400 Loss 0.694001\n",
      "Step 500 Loss 0.691938\n",
      "Step 600 Loss 0.692326\n",
      "New Data, epoch 68\n",
      "Step 0 Loss 0.691512\n",
      "Step 100 Loss 0.696448\n",
      "Step 200 Loss 0.694084\n",
      "Step 300 Loss 0.69536\n",
      "Step 400 Loss 0.691608\n",
      "Step 500 Loss 0.696214\n",
      "Step 600 Loss 0.691504\n",
      "New Data, epoch 69\n",
      "Step 0 Loss 0.685269\n",
      "Step 100 Loss 0.696996\n",
      "Step 200 Loss 0.693906\n",
      "Step 300 Loss 0.693074\n",
      "Step 400 Loss 0.688788\n",
      "Step 500 Loss 0.698828\n",
      "Step 600 Loss 0.693545\n",
      "New Data, epoch 70\n",
      "Step 0 Loss 0.695773\n",
      "Step 100 Loss 0.693061\n",
      "Step 200 Loss 0.693717\n",
      "Step 300 Loss 0.691343\n",
      "Step 400 Loss 0.694383\n",
      "Step 500 Loss 0.693867\n",
      "Step 600 Loss 0.692864\n",
      "New Data, epoch 71\n",
      "Step 0 Loss 0.688861\n",
      "Step 100 Loss 0.693041\n",
      "Step 200 Loss 0.69493\n",
      "Step 300 Loss 0.691952\n",
      "Step 400 Loss 0.693328\n",
      "Step 500 Loss 0.697059\n",
      "Step 600 Loss 0.693534\n",
      "New Data, epoch 72\n",
      "Step 0 Loss 0.681722\n",
      "Step 100 Loss 0.688375\n",
      "Step 200 Loss 0.698387\n",
      "Step 300 Loss 0.691579\n",
      "Step 400 Loss 0.692703\n",
      "Step 500 Loss 0.69356\n",
      "Step 600 Loss 0.69274\n",
      "New Data, epoch 73\n",
      "Step 0 Loss 0.686689\n",
      "Step 100 Loss 0.694925\n",
      "Step 200 Loss 0.695069\n",
      "Step 300 Loss 0.693383\n",
      "Step 400 Loss 0.694271\n",
      "Step 500 Loss 0.693431\n",
      "Step 600 Loss 0.694017\n",
      "New Data, epoch 74\n",
      "Step 0 Loss 0.692694\n",
      "Step 100 Loss 0.694298\n",
      "Step 200 Loss 0.693287\n",
      "Step 300 Loss 0.692221\n",
      "Step 400 Loss 0.695404\n",
      "Step 500 Loss 0.693401\n",
      "Step 600 Loss 0.693429\n",
      "New Data, epoch 75\n",
      "Step 0 Loss 0.692335\n",
      "Step 100 Loss 0.693946\n",
      "Step 200 Loss 0.692978\n",
      "Step 300 Loss 0.695413\n",
      "Step 400 Loss 0.68905\n",
      "Step 500 Loss 0.693265\n",
      "Step 600 Loss 0.692437\n",
      "New Data, epoch 76\n",
      "Step 0 Loss 0.686685\n",
      "Step 100 Loss 0.693376\n",
      "Step 200 Loss 0.693378\n",
      "Step 300 Loss 0.694808\n",
      "Step 400 Loss 0.69152\n",
      "Step 500 Loss 0.692567\n",
      "Step 600 Loss 0.699892\n",
      "New Data, epoch 77\n",
      "Step 0 Loss 0.692105\n",
      "Step 100 Loss 0.694587\n",
      "Step 200 Loss 0.694515\n",
      "Step 300 Loss 0.692043\n",
      "Step 400 Loss 0.695817\n",
      "Step 500 Loss 0.692548\n",
      "Step 600 Loss 0.693502\n",
      "New Data, epoch 78\n",
      "Step 0 Loss 0.687529\n",
      "Step 100 Loss 0.696497\n",
      "Step 200 Loss 0.689747\n",
      "Step 300 Loss 0.694105\n",
      "Step 400 Loss 0.693734\n",
      "Step 500 Loss 0.690288\n",
      "Step 600 Loss 0.693113\n",
      "New Data, epoch 79\n",
      "Step 0 Loss 0.683999\n",
      "Step 100 Loss 0.693699\n",
      "Step 200 Loss 0.693927\n",
      "Step 300 Loss 0.695057\n",
      "Step 400 Loss 0.69303\n",
      "Step 500 Loss 0.691965\n",
      "Step 600 Loss 0.698194\n",
      "New Data, epoch 80\n",
      "Step 0 Loss 0.694012\n",
      "Step 100 Loss 0.693555\n",
      "Step 200 Loss 0.692785\n",
      "Step 300 Loss 0.691027\n",
      "Step 400 Loss 0.692516\n",
      "Step 500 Loss 0.693455\n",
      "Step 600 Loss 0.693309\n",
      "New Data, epoch 81\n",
      "Step 0 Loss 0.696043\n",
      "Step 100 Loss 0.692921\n",
      "Step 200 Loss 0.691154\n",
      "Step 300 Loss 0.693684\n",
      "Step 400 Loss 0.697144\n",
      "Step 500 Loss 0.691003\n",
      "Step 600 Loss 0.695649\n",
      "New Data, epoch 82\n",
      "Step 0 Loss 0.693299\n",
      "Step 100 Loss 0.691902\n",
      "Step 200 Loss 0.692695\n",
      "Step 300 Loss 0.693451\n",
      "Step 400 Loss 0.693512\n",
      "Step 500 Loss 0.691225\n",
      "Step 600 Loss 0.697088\n",
      "New Data, epoch 83\n",
      "Step 0 Loss 0.681856\n",
      "Step 100 Loss 0.699242\n",
      "Step 200 Loss 0.69722\n",
      "Step 300 Loss 0.692916\n",
      "Step 400 Loss 0.692658\n",
      "Step 500 Loss 0.693203\n",
      "Step 600 Loss 0.694333\n",
      "New Data, epoch 84\n",
      "Step 0 Loss 0.691868\n",
      "Step 100 Loss 0.693468\n",
      "Step 200 Loss 0.692496\n",
      "Step 300 Loss 0.693013\n",
      "Step 400 Loss 0.693793\n",
      "Step 500 Loss 0.690594\n",
      "Step 600 Loss 0.689515\n",
      "New Data, epoch 85\n",
      "Step 0 Loss 0.688308\n",
      "Step 100 Loss 0.691519\n",
      "Step 200 Loss 0.69352\n",
      "Step 300 Loss 0.69286\n",
      "Step 400 Loss 0.698921\n",
      "Step 500 Loss 0.693479\n",
      "Step 600 Loss 0.697201\n",
      "New Data, epoch 86\n",
      "Step 0 Loss 0.689139\n",
      "Step 100 Loss 0.690193\n",
      "Step 200 Loss 0.693285\n",
      "Step 300 Loss 0.693163\n",
      "Step 400 Loss 0.692695\n",
      "Step 500 Loss 0.6918\n",
      "Step 600 Loss 0.694958\n",
      "New Data, epoch 87\n",
      "Step 0 Loss 0.691122\n",
      "Step 100 Loss 0.693768\n",
      "Step 200 Loss 0.693205\n",
      "Step 300 Loss 0.693378\n",
      "Step 400 Loss 0.694178\n",
      "Step 500 Loss 0.693403\n",
      "Step 600 Loss 0.693454\n",
      "New Data, epoch 88\n",
      "Step 0 Loss 0.697174\n",
      "Step 100 Loss 0.691062\n",
      "Step 200 Loss 0.695388\n",
      "Step 300 Loss 0.693731\n",
      "Step 400 Loss 0.694146\n",
      "Step 500 Loss 0.693869\n",
      "Step 600 Loss 0.690592\n",
      "New Data, epoch 89\n",
      "Step 0 Loss 0.691337\n",
      "Step 100 Loss 0.696234\n",
      "Step 200 Loss 0.683534\n",
      "Step 300 Loss 0.693047\n",
      "Step 400 Loss 0.693355\n",
      "Step 500 Loss 0.692774\n",
      "Step 600 Loss 0.695899\n",
      "New Data, epoch 90\n",
      "Step 0 Loss 0.693406\n",
      "Step 100 Loss 0.692214\n",
      "Step 200 Loss 0.693846\n",
      "Step 300 Loss 0.693483\n",
      "Step 400 Loss 0.693245\n",
      "Step 500 Loss 0.693332\n",
      "Step 600 Loss 0.693078\n",
      "New Data, epoch 91\n",
      "Step 0 Loss 0.691991\n",
      "Step 100 Loss 0.693797\n",
      "Step 200 Loss 0.702333\n",
      "Step 300 Loss 0.692612\n",
      "Step 400 Loss 0.692594\n",
      "Step 500 Loss 0.693759\n",
      "Step 600 Loss 0.693336\n",
      "New Data, epoch 92\n",
      "Step 0 Loss 0.691363\n",
      "Step 100 Loss 0.693371\n",
      "Step 200 Loss 0.691171\n",
      "Step 300 Loss 0.693371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400 Loss 0.691426\n",
      "Step 500 Loss 0.692098\n",
      "Step 600 Loss 0.691272\n",
      "New Data, epoch 93\n",
      "Step 0 Loss 0.685584\n",
      "Step 100 Loss 0.694189\n",
      "Step 200 Loss 0.688721\n",
      "Step 300 Loss 0.691734\n",
      "Step 400 Loss 0.688839\n",
      "Step 500 Loss 0.694007\n",
      "Step 600 Loss 0.689071\n",
      "New Data, epoch 94\n",
      "Step 0 Loss 0.691329\n",
      "Step 100 Loss 0.698077\n",
      "Step 200 Loss 0.693128\n",
      "Step 300 Loss 0.692531\n",
      "Step 400 Loss 0.693421\n",
      "Step 500 Loss 0.693174\n",
      "Step 600 Loss 0.693033\n",
      "New Data, epoch 95\n",
      "Step 0 Loss 0.6902\n",
      "Step 100 Loss 0.697924\n",
      "Step 200 Loss 0.69257\n",
      "Step 300 Loss 0.689247\n",
      "Step 400 Loss 0.690422\n",
      "Step 500 Loss 0.691907\n",
      "Step 600 Loss 0.693213\n",
      "New Data, epoch 96\n",
      "Step 0 Loss 0.688144\n",
      "Step 100 Loss 0.69338\n",
      "Step 200 Loss 0.692804\n",
      "Step 300 Loss 0.692753\n",
      "Step 400 Loss 0.691547\n",
      "Step 500 Loss 0.692641\n",
      "Step 600 Loss 0.693708\n",
      "New Data, epoch 97\n",
      "Step 0 Loss 0.68136\n",
      "Step 100 Loss 0.694522\n",
      "Step 200 Loss 0.691783\n",
      "Step 300 Loss 0.692247\n",
      "Step 400 Loss 0.693607\n",
      "Step 500 Loss 0.692022\n",
      "Step 600 Loss 0.690129\n",
      "New Data, epoch 98\n",
      "Step 0 Loss 0.683555\n",
      "Step 100 Loss 0.695079\n",
      "Step 200 Loss 0.694166\n",
      "Step 300 Loss 0.692503\n",
      "Step 400 Loss 0.69387\n",
      "Step 500 Loss 0.694753\n",
      "Step 600 Loss 0.69322\n",
      "New Data, epoch 99\n",
      "Step 0 Loss 0.689938\n",
      "Step 100 Loss 0.695194\n",
      "Step 200 Loss 0.692888\n",
      "Step 300 Loss 0.698713\n",
      "Step 400 Loss 0.693136\n",
      "Step 500 Loss 0.691874\n",
      "Step 600 Loss 0.693035\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117a46358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x, y = generateData()\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "        print(\"New Data, epoch\", epoch_idx)\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "            \n",
    "            batchX = x[:,start_idx: end_idx]\n",
    "            batchY = y[:,start_idx: end_idx]\n",
    "            \n",
    "            _total_loss, _train_loss, _current_state, _predictions_series = sess.run(\n",
    "            [total_loss,train_step,current_state,predictions_series],\n",
    "            feed_dict = { batchX_placeholder:batchX,\n",
    "                        batchY_placeholder:batchY,\n",
    "                        init_state:_current_state})\n",
    "            loss_list.append(_total_loss)\n",
    "            \n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\", batch_idx, \"Loss\", _total_loss)\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "                \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program produces a series of plots that show the input data being learnt and repeated x samples after the input is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN API\n",
    "\n",
    "\n",
    "Going back to the forward pass, we can make use of tensorflows APIs, we first begin by modifying the tanh function and then appending and reshaping code and just write tf.contrib.rnn.BasicRNNCell(state_size) and add tf.contrib.rnn(cell, inputs, init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing the long for loop iterating through the length of the input. The tf.nn.rnn unrolls the RNN and creates the graph automatically, so the loop can be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Forward Pass\n",
    "\n",
    "#Modify Unpack columns\n",
    "#inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
    "#labels_series = tf.unstack(batchY_placeholder, axis=1)\n",
    "\n",
    "# Replace\n",
    "\n",
    "# Forward pass\n",
    "#current_state = init_state\n",
    "#states_series = []\n",
    "#for current_input in inputs_series:\n",
    "#    current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "#    input_and_state_concatenated = tf.concat(1, [current_input, current_state])  # Increasing number of columns\n",
    "\n",
    "#    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
    "#    states_series.append(next_state)\n",
    "#    current_state = next_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'size_splits' has DataType float32 not in list of allowed values: int32, int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-87a3f17d2bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minputs_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncated_backprop_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchX_placeholder\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabels_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchY_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(value, num_or_size_splits, axis, num, name)\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0msplit_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0mnum_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_split_v\u001b[0;34m(value, size_splits, split_dim, num_split, name)\u001b[0m\n\u001b[1;32m   3334\u001b[0m   result = _op_def_lib.apply_op(\"SplitV\", value=value,\n\u001b[1;32m   3335\u001b[0m                                 \u001b[0msize_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3336\u001b[0;31m                                 num_split=num_split, name=name)\n\u001b[0m\u001b[1;32m   3337\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    588\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    589\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 61\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'size_splits' has DataType float32 not in list of allowed values: int32, int64"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs_series = tf.split(truncated_backprop_length, batchX_placeholder , axis = 1)\n",
    "labels_series = tf.unstack(batchY_placeholder, axis = 1)\n",
    "\n",
    "\n",
    "# Forward passes\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell, inputs_series, init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and biases can also be removed. Most of the RNN is woven into the Basic RNN cell structure. Split is used instead of unstack the tf.nn.dynamic_rnn accepts the input shape of [batch_size, input_size], input size is simply one. Split keeps the single dimension that we are using in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is added the code is a little easier to implement. But we can improve on this using an LSTM network. The LSTM will allow us to extend our truncated_backprop_length out further without introducing the vanishing gradient problem that simple RNNs are prone to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM has an internal cell state and a hidden state. To introduce these into the program _current_state is replaced with two lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_current_cell_state = np.zeros((batch_size, state_size))\n",
    "_current_hidden_state = np.zeros((batch_size, state_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs use LSTMStateTuple to store the data. We therefore create placeholders for cell state and hidden state and place them in the tuple,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "hidden_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "init_state = tf.contrib.rnn.LSTMStateTuple(cell_state, hidden_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn BasicRNNCell into BasicLSTMCell, and then the program can calculate the results of the tuple. This is then seperated out after calculated and is supplied to the placeholders in the run function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension must be 2 but is 3 for 'transpose_5' (op: 'Transpose') with input shapes: [5,1], [3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension must be 2 but is 3 for 'transpose_5' (op: 'Transpose') with input shapes: [5,1], [3].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-d330232bac11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Forward passes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mstates_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mlogits_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates_series\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Broadcasted addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m   3719\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3720\u001b[0m   \"\"\"\n\u001b[0;32m-> 3721\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transpose\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3722\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension must be 2 but is 3 for 'transpose_5' (op: 'Transpose') with input shapes: [5,1], [3]."
     ]
    }
   ],
   "source": [
    "#Result\n",
    "\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "\n",
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "cell_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "hidden_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "init_state = tf.contrib.rnn.LSTMStateTuple(cell_state, hidden_state)\n",
    "\n",
    "# Unpack columns\n",
    "inputs_series = tf.split( batchX_placeholder, truncated_backprop_length, 1)\n",
    "labels_series = tf.unstack(batchY_placeholder, axis=1)\n",
    "\n",
    "# Forward passes\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(state_size, state_is_tuple=True)\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell, inputs_series, init_state)\n",
    "\n",
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\n",
    "\n",
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = generateData()\n",
    "        _current_cell_state = np.zeros((batch_size, state_size))\n",
    "        _current_hidden_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY,\n",
    "                    cell_state: _current_cell_state,\n",
    "                    hidden_state: _current_hidden_state\n",
    "\n",
    "                })\n",
    "\n",
    "            _current_cell_state, _current_hidden_state = _current_state\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx, \"Batch loss\", _total_loss)\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further extend a simple LSTM into multilayers by changing the LSTMTuple line to a more generic _current_state line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_current_state = np.zeros((num_layers, 2, batch_size, state_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then modify the layer size to what ever length we like. Init state is then converted from LSTMTuple back to tf.placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, batch_size, state_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stateTuple is then created to encompass the multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_per_layer_list = tf.unstack(init_state, axis=0)\n",
    "rnn_tuple_state = tuple(\n",
    "    [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "     for idx in range(num_layers)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the forward pass introduces a MultiRNNCell which links the layers together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension must be 2 but is 3 for 'transpose_6' (op: 'Transpose') with input shapes: [5,1], [3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension must be 2 but is 3 for 'transpose_6' (op: 'Transpose') with input shapes: [5,1], [3].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-c5131a17e58b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstates_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_tuple_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m   3719\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3720\u001b[0m   \"\"\"\n\u001b[0;32m-> 3721\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transpose\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3722\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension must be 2 but is 3 for 'transpose_6' (op: 'Transpose') with input shapes: [5,1], [3]."
     ]
    }
   ],
   "source": [
    "# Forward passes\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(state_size, state_is_tuple=True)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell, inputs_series, initial_state=rnn_tuple_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't split scalars for 'split_17' (op: 'SplitV') with input shapes: [], [5,15], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Can't split scalars for 'split_17' (op: 'SplitV') with input shapes: [], [5,15], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-866a7b12ed23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Unpack columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0minputs_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncated_backprop_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchX_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mlabels_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchY_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(value, num_or_size_splits, axis, num, name)\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0msplit_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0mnum_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_split_v\u001b[0;34m(value, size_splits, split_dim, num_split, name)\u001b[0m\n\u001b[1;32m   3334\u001b[0m   result = _op_def_lib.apply_op(\"SplitV\", value=value,\n\u001b[1;32m   3335\u001b[0m                                 \u001b[0msize_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3336\u001b[0;31m                                 num_split=num_split, name=name)\n\u001b[0m\u001b[1;32m   3337\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't split scalars for 'split_17' (op: 'SplitV') with input shapes: [], [5,15], []."
     ]
    }
   ],
   "source": [
    "#Full program\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "num_layers = 3\n",
    "\n",
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "batchX_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, batch_size, state_size])\n",
    "\n",
    "state_per_layer_list = tf.unstack(init_state, axis=0)\n",
    "rnn_tuple_state = tuple(\n",
    "    [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "     for idx in range(num_layers)]\n",
    ")\n",
    "\n",
    "\n",
    "# Unpack columns\n",
    "\n",
    "inputs_series = tf.split(truncated_backprop_length,batchX_placeholder)\n",
    "labels_series = tf.unstack(batchY_placeholder, axis=1)\n",
    "\n",
    "# Forward passes\n",
    "cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(state_size, state_is_tuple=True, reuse=True) for _ in range(num_layers)], state_is_tuple=True)\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell, inputs_series, initial_state=rnn_tuple_state)\n",
    "\n",
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\n",
    "\n",
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = generateData()\n",
    "\n",
    "        _current_state = np.zeros((num_layers, 2, batch_size, state_size))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY,\n",
    "                    init_state: _current_state\n",
    "                })\n",
    "\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx, \"Batch loss\", _total_loss)\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to get this code to a neat fully functional LSTM we fully incorporate dynamic_rnn into the program. By doing this we don't need to split the inputs and labels into a list. Inputs_series and label_series are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-7f71e0b16efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstates_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchX_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_tuple_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstates_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    718\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   2621\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2454\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2456\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2404\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m-> 2406\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2408\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    703\u001b[0m           skip_conditionals=True)\n\u001b[1;32m    704\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;31m# Pack state if using state tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_checked_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"basic_rnn_cell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       output = self._activation(\n\u001b[0;32m--> 122\u001b[0;31m           _linear([inputs, state], self._num_units, True))\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m_linear\u001b[0;34m(args, output_size, bias, bias_start)\u001b[0m\n\u001b[1;32m   1026\u001b[0m   \u001b[0;31m# Calculate the total size of arguments on dimension 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0mtotal_arg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1026\u001b[0m   \u001b[0;31m# Calculate the total size of arguments on dimension 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0mtotal_arg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "states_series, current_state = tf.nn.dynamic_rnn(cell, tf.expand_dims(batchX_placeholder, -1), initial_state=rnn_tuple_state)\n",
    "states_series = tf.reshape(states_series, [-1, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5261aa7e3ada>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mstates_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchX_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_tuple_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mstates_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    718\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   2621\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2454\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2456\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2404\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m-> 2406\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2408\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    703\u001b[0m           skip_conditionals=True)\n\u001b[1;32m    704\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;31m# Pack state if using state tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    951\u001b[0m                 state, [0, cur_state_pos], [-1, cell.state_size])\n\u001b[1;32m    952\u001b[0m             \u001b[0mcur_state_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m           \u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m           \u001b[0mnew_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m     new_states = (tuple(new_states) if self._state_is_tuple else\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Long short-term memory cell (LSTM).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_checked_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"basic_lstm_cell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m       \u001b[0;31m# Parameters of gates are concatenated into one multiply for efficiency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_is_tuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timothy/anaconda/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m_checked_scope\u001b[0;34m(cell, scope, reuse, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;34m\"To share the weights of an RNNCell, simply \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;34m\"reuse it in your second calculation, or create a new one with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \"the argument reuse=True.\" % (scope_name, type(cell).__name__))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Everything is OK.  Update the cell's scope and yield it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True."
     ]
    }
   ],
   "source": [
    "#Full program\n",
    "\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "num_layers = 3\n",
    "\n",
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, batch_size, state_size])\n",
    "\n",
    "state_per_layer_list = tf.unstack(init_state, axis=0)\n",
    "rnn_tuple_state = tuple(\n",
    "    [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "     for idx in range(num_layers)]\n",
    ")\n",
    "\n",
    "# Forward passes\n",
    "\n",
    "\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(state_size, state_is_tuple=True, ) for _ in range(num_layers)], state_is_tuple=True)\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell, tf.expand_dims(batchX_placeholder, -1), initial_state=rnn_tuple_state)\n",
    "states_series = tf.reshape(states_series, [-1, state_size])\n",
    "\n",
    "logits = tf.matmul(states_series, W2) + b2 #Broadcasted addition\n",
    "labels = tf.reshape(batchY_placeholder, [-1])\n",
    "\n",
    "logits_series = tf.unstack(tf.reshape(logits, [batch_size, truncated_backprop_length, num_classes]), axis=1)\n",
    "predictions_series = [tf.nn.softmax(logit) for logit in logits_series]\n",
    "\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\n",
    "\n",
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = generateData()\n",
    "\n",
    "        _current_state = np.zeros((num_layers, 2, batch_size, state_size))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY,\n",
    "                    init_state: _current_state\n",
    "                })\n",
    "\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx, \"Batch loss\", _total_loss)\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
